{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Read the data from the h5py file and understand the train/test**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import h5py as h5fy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h5f=h5fy.File('../input/street-view-house-nos-h5-file/SVHN_single_grey1.h5','r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h5f.keys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = h5f['X_train'][:]\ny_train = h5f['y_train'][:]\nX_test = h5f['X_test'][:]\ny_test = h5f['y_test'][:]\nprint('X_train' ,X_train.shape)\nprint('y_train' ,y_train.shape)\nprint('X_test' ,X_test.shape)\nprint('y_test' ,y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Observation about dataset:****\n\n1) We have a tensor with 32*32*n Dimension.(n is number of rows in the tensor while each row as 32*32 matrix)\n\n2) We have training values interms of pixel seems. Each 32*32 defines an image of number.\n\n![](http://)3) We have to predict the number (0 to 9) which is our target variable. Our target is multi-level classification so it is evident that while, input shape is 32*32*n(each row of tensor being 32*32 pixel) and our o/p or target has only one column i.e. from 0 to 9."},{"metadata":{},"cell_type":"markdown","source":"**let's plot one input row of our tensor**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.imshow(X_train[9][:][:])\nprint('Label: ', y_train[9])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Pre-processing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's flatten in out and convert the 3D into 2D tumpy array\nX_train = X_train.reshape((X_train.shape[0], -1))\nX_test = X_test.reshape((X_test.shape[0], -1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting y data into categorical (one-hot encoding)\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X_train\",X_train.shape,\" X_test\", X_test.shape, 'y_train',y_train.shape, 'y_test',y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's Build a NN "},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.layers import BatchNormalization, Dropout","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try with a Vanila model:\n1) Sequential model \n2) relu as Activation Function for input and Hidden Layer and softmax as Activation Function for our output layer\n3) With SGD optimizer and loss function categorical_crossentropy\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_vanila_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, )))                  \n    model.add(Activation('relu'))    \n    model.add(Dense(50))                   \n    model.add(Activation('relu'))    \n    model.add(Dense(50))                   \n    model.add(Activation('relu'))    \n    model.add(Dense(50))                    \n    model.add(Activation('relu'))    \n    model.add(Dense(10))\n    model.add(Activation('softmax'))\n    \n    sgd = optimizers.SGD(lr = 0.001)\n    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    model = create_vanila_model()\n    history = model.fit(X_train, y_train, batch_size=200, epochs = 200,verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just 13% of accuracy is not acceptable. Let's use BatchNormalization."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_batchnorm_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, )))\n    model.add(BatchNormalization())                    \n    model.add(Activation('relu'))    \n    model.add(Dense(50))\n    model.add(BatchNormalization())                    \n    model.add(Activation('relu'))    \n    model.add(Dense(50))\n    model.add(BatchNormalization())                    \n    model.add(Activation('relu'))    \n    model.add(Dense(50))\n    model.add(BatchNormalization())                    \n    model.add(Activation('relu'))    \n    model.add(Dense(10))\n    model.add(Activation('softmax'))\n    \n    sgd = optimizers.SGD(lr = 0.001)\n    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    model = create_batchnorm_model()\n    history = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Surely 75% accuracy is not acceptable. But the significant of Batchnormalization is immense here. Let's try tuning our hyper parameters a bit further but keep the very nature of our NN intact."},{"metadata":{},"cell_type":"markdown","source":"Let's use a kernel initializer(The fancy term use just for initilizing weights :))"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mlp_kernel_init_Batch_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(10, kernel_initializer='he_normal'))\n    model.add(Activation('softmax'))\n    \n    sgd = optimizers.SGD(lr = 0.001)\n    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_batchnorm_model()\nhistory = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's evaluate with test data:\nresults = model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Seems just adding a kernel_initializer has no improvement. Let's try adding a dropout to train our model little better.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bn_ki_dropout_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dropout(0.2))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(10, kernel_initializer='he_normal'))\n    model.add(Activation('softmax'))\n    sgd = optimizers.SGD(lr = 0.001)\n    model.compile(optimizer = sgd , loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = bn_ki_dropout_model()\nhistory = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's evaluate with test data:\nresults = model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation : This shows that **we need all the features and can't afford to ignore any features**. Let's go back and don't use dropout. Instead may be we can try a different optimizer altogether."},{"metadata":{"trusted":true},"cell_type":"code","source":"def adam_optimizer_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(10, kernel_initializer='he_normal'))\n    model.add(Activation('softmax'))\n    \n    adam = optimizers.Adam(lr = 0.001)\n    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = adam_optimizer_model()\nhistory = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's evaluate with test data:\nresults = model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So with **Adam as optimizer we got an over-fit model**. Let's go back to dropout strategy with Adam. would be worth to observe!"},{"metadata":{"trusted":true},"cell_type":"code","source":"def adam_with_dropout_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dropout(0.2))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(10, kernel_initializer='he_normal'))\n    model.add(Activation('softmax'))\n    \n    adam = optimizers.Adam(lr = 0.001)\n    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = adam_with_dropout_model()\nhistory = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's evaluate with test data:\nresults = model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation : Adam optimizer with dropout strategy looks a bit better as our test accuracy is ~76%.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sgd_momentum_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(10, kernel_initializer='he_normal'))\n    model.add(Activation('softmax'))\n    sdgm = optimizers.SGD(lr=0.01, momentum=0.9)\n    model.compile(optimizer = sdgm, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = sgd_momentum_model()\nhistory = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's evaluate with test data:\nresults = model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Observation : With SGD momentum optimizer model looks a bit better as our test accuracy is ~80%.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sgd_momentum_ki_u_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='uniform'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='uniform'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dense(50, kernel_initializer='uniform'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='uniform'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(10, kernel_initializer='uniform'))\n    model.add(Activation('softmax'))\n    sdgm = optimizers.SGD(lr=0.01, momentum=0.9)\n    model.compile(optimizer = sdgm, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = sgd_momentum_ki_hu_model()\nhistory = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's evaluate with test data:\nresults = model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation : With SGD momentum optimizer and uniform kernel initializer. Seems not better than SGD momentum + he uniform KI."},{"metadata":{"trusted":true},"cell_type":"code","source":"def adam_ki_hn_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(10, kernel_initializer='he_normal'))\n    model.add(Activation('softmax'))\n    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = adam_ki_hn_model()\nhistory = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's evaluate with test data:\nresults = model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation : With adam optimizer, model looks a bit better as our test accuracy is > 80%"},{"metadata":{},"cell_type":"markdown","source":"Let's Test by changing our network with a new hidden layer and observe if we get better result!"},{"metadata":{"trusted":true},"cell_type":"code","source":"def adam_ki_hn_model_plus_one_Hiddnen():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu')) \n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(10, kernel_initializer='he_normal'))\n    model.add(Activation('softmax'))\n    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = adam_ki_hn_model_plus_one_Hiddnen()\nhistory = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's evaluate with test data:\nresults = model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation : As we can see here, our model accuracy slightly decreased after addition of new hidden layer. This can be attributed to the model is short of started memorizing the trained dataset and will not generalize well. So let's don't change the structure of our NN**"},{"metadata":{},"cell_type":"markdown","source":"Observation : We have the accuracy of 80%. Not tried with  Image Augmentation. This might have helped. Please do let me know if you have any comments on that!"},{"metadata":{"trusted":true},"cell_type":"code","source":"def adam2_with_dropout_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dropout(0.5))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(10, kernel_initializer='he_normal'))\n    model.add(Activation('softmax'))\n    \n    adam = optimizers.Adam(lr = 0.01 , beta_1=0.9 , decay =0)\n    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = adam2_with_dropout_model()\nhistory = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation** : With the above hyper parameter tuinig our best model would have accuracy 80%."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}